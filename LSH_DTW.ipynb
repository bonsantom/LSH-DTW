{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Equal length Time Series Classifier using Locality Sensitive Hashing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vectors(minimum=0, maximum=1, vectors_shape=1, num_vectors=1, seed=0):\n",
    "    \"\"\"\n",
    "    Generate random vectors (or chuncks) to separate data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    minimum: \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vehicles_now_df: cleaned pandas dataframe\n",
    "    rides_now_df: cleaned pandas dataframe\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    return np.random.uniform(minimum, maximum, (num_vectors, vectors_shape)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_into_bins(data, random_vectors):\n",
    "    \"\"\" \n",
    "    Separate data into bins\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: (pandas DataFrame) corresponding to the dataset you wwant to train\n",
    "    random_vectors: (numpy array) necessary to separate dataset into bins\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k-dimensional binary array showing if the data point is over or under the vector \n",
    "    \"\"\"\n",
    "    return np.array(data.dot(random_vectors) >= 0, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_table(bin_indices):\n",
    "    \"\"\"\n",
    "    Update 'table' so that 'table[i]' is the list of document ids with bin index equal to i.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bin_indices: integer representation of bin indexes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary containing integer representation data\n",
    "    \"\"\" \n",
    "    table = dict()\n",
    "    for data_index, bin_index in enumerate(bin_indices): \n",
    "        if bin_index not in table:\n",
    "            table[bin_index] = []\n",
    "        table[bin_index].append(data_index) #indice del bin\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSH(data, num_vectors, seed):\n",
    "    \"\"\"\n",
    "    Locality Sensitive Hashing training function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe or array used to train\n",
    "    num_vectors: number of chunks or bins\n",
    "    seed: seed to replicate values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary containing integer representation data\n",
    "    \"\"\"\n",
    "    data = data.reshape(len(data),-1)\n",
    "    random_vectors = generate_random_vectors(vectors_shape=data[0].shape[0], num_vectors=num_vectors, seed=seed)\n",
    "    bin_index_bits = partition_into_bins(data, random_vectors)\n",
    "    powers_of_two = 1 << np.arange(num_vectors-1, -1, -1)\n",
    "    bin_indices = bin_index_bits.dot(powers_of_two)\n",
    "    train_table = bins_table(bin_indices)\n",
    "    trained_LSH = dict()\n",
    "    trained_LSH['bin_index_bits'] = bin_index_bits\n",
    "    trained_LSH['bin_indices'] = bin_indices\n",
    "    trained_LSH['table'] = train_table\n",
    "    trained_LSH['random_vectors'] = random_vectors\n",
    "    return trained_LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW_models(X_train, y_train, trained_LSH):\n",
    "    \"\"\"\n",
    "    Fits a k-NN model for every chunk\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: (array) corresponding to the data features for the model to learn\n",
    "    y_train: (array) corresponding to the data labels for the model to learn\n",
    "    trained_LSH: (dictionary)  containing integer representation of data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary containing integer representation data and trained models \n",
    "    for every bin or chuck\n",
    "    \"\"\"\n",
    "    table_model = dict()\n",
    "    for i in trained_LSH['table'].keys():\n",
    "        X_train_ts = to_time_series_dataset(X_train[trained_LSH['table'][i]])\n",
    "        y_train_ts = y_train[trained_LSH['table'][i]]\n",
    "        knn = KNeighborsTimeSeriesClassifier(n_neighbors=2)\n",
    "        table_model[i] = knn.fit(X_train_ts, y_train_ts)\n",
    "    return table_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW_predict(table_LSH, X_test):\n",
    "    \"\"\"\n",
    "    Function used to predict X_test array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trained_LSH: (dictionary)  containing integer representation of data\n",
    "    X_test: (array) corresponding to the data features for the model to predict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array with predicted labels\n",
    "    \n",
    "    \"\"\"\n",
    "    num_vectors = len(table_LSH['bin_index_bits'][0])\n",
    "    X_test = X_test.reshape(len(X_test),-1)\n",
    "    \n",
    "    bin_index_bits_test = partition_into_bins(X_test, table_LSH['random_vectors'])\n",
    "    powers_of_two = 1 << np.arange(num_vectors-1, -1, -1)\n",
    "    bin_indices_test = bin_index_bits_test.dot(powers_of_two)\n",
    "    test_table = bins_table(bin_indices_test)\n",
    "    y_hat = []\n",
    "    for i in table_LSH['table'].keys():\n",
    "        y_hat += list(zip(test_table[i], table_LSH['models'][i].predict(X_test[test_table[i]]))) # falta posicion de la prediccion\n",
    "    y_hat = np.array(y_hat)\n",
    "    return y_hat[y_hat[:,0].argsort()][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW_classifier(X_train, y_train, num_vectors, seed):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: (array) corresponding to the data features for the model to learn\n",
    "    y_train: (array) corresponding to the data labels for the model to learn\n",
    "    num_vectors: number of chunks or bins\n",
    "    seed: seed to replicate values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary containing integer representation data and trained models \n",
    "    for every bin or chuck\n",
    "    \"\"\"\n",
    "    trained_LSH = train_LSH(X_train, num_vectors, seed)\n",
    "    trained_LSH['models'] = DTW_models(X_train, y_train, trained_LSH)\n",
    "    return trained_LSH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
